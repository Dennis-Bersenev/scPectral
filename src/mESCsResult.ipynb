{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source publication for dataset: \n",
    "https://www.nature.com/articles/s41467-018-02866-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = 456\n",
    "n_genes = 100\n",
    "PWscores = np.zeros((n_cells, n_genes, n_genes))\n",
    "\n",
    "for i in range(n_cells):\n",
    "    path = \"./../data/TEsmESC/geneXgene{i}.csv\".format(i=(i+1))\n",
    "    df_temp = pd.read_csv(path)\n",
    "    PWscores[i, :, :] = np.copy(df_temp.values)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing TE scores across cells (over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat_fig = sn.heatmap(PWscores[0, :, :], square=True)\n",
    "    # as_file = heat_fig.get_figure()\n",
    "    # path = \"./../out/cell{i}.png\".format(i = i + 1)\n",
    "    # as_file.savefig(path)\n",
    "    # ax.tick_params(left=False, bottom=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The informative gene pathway extraction algorithm\n",
    "(For now implemented as just an easy intuitive iterative solution, maybe later will attempt a numpy/C++ acceleration!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: right now it is just greedy bfs, but I want paths to be able to branch and for multiple nodes to all connect to a common node\n",
    "# eg. if you get 3-> 59, 18 -> 59 that outta be shown as 3&18 -> 59 (somehow)\n",
    "\"\"\"\n",
    "TODO: just build the incidence matrix as you go? each iteration adds a new set 'layer' of edges\n",
    "--> see notes !\n",
    "implement this proper (will write another function)\n",
    "\"\"\"\n",
    "def get_hyperedge_set(cell_index):\n",
    "\n",
    "    cell_nw = PWscores[cell_index, :, :]\n",
    "    \n",
    "    # init\n",
    "    paths = []\n",
    "    max_itrs = n_genes\n",
    "    itrs = 0\n",
    "    tolerance = np.max(cell_nw) * 0.2 # all scores within whatever % of the max\n",
    "    max_path_len = 0\n",
    "    max_path_len_prev = -1\n",
    "\n",
    "    for i in range(n_genes): #row is the predecessor node\n",
    "        index_max = np.argmax(cell_nw[i, :]) # index of max column is the successor (incident) node\n",
    "        \n",
    "        if cell_nw[i, index_max] > tolerance:\n",
    "            paths.append([i, index_max])\n",
    "\n",
    "    # update\n",
    "    while (max_itrs > itrs) and (max_path_len > max_path_len_prev):\n",
    "        max_path_len_prev = max_path_len\n",
    "        new_paths = deepcopy(paths) \n",
    "        for p in range(len(paths)):\n",
    "            path = paths[p]\n",
    "            if len(path) >= max_path_len:\n",
    "                row = path[-1]\n",
    "                index_max = np.argmax(cell_nw[row, :])\n",
    "                \n",
    "                if (cell_nw[row, index_max] > tolerance) and (index_max not in new_paths[p]):\n",
    "                    new_paths[p].append(index_max)\n",
    "\n",
    "        max_path_len = max( len(x) for x in new_paths )\n",
    "        itrs += 1\n",
    "        paths = deepcopy(new_paths)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the hypergraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: a simple canonical edge has the form of tail->head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merges tails with common heads to identify all the multi-arity relations to represent as hedges and updates the supplied incidence matrix with these final hedges.\n",
    "_edges: a list of tuples (tail_set, head_set) of all the initial single-tailed edges (note: they are sets, not lists!)\n",
    "B: a list of n_gene-sized integer numpy arrays to represent the incidence matrix to update.\n",
    "TODO: Test and be especially make sure Python's memory system isn't doing any weird reference copying and producing the wrong stuff!\n",
    "Note, all set operations return copies, so this SHOULD be okay.\n",
    "\"\"\"\n",
    "def merge_edges(_edges, B):\n",
    "    \n",
    "    # init\n",
    "    edges = deepcopy(_edges)\n",
    "    m = len(edges)\n",
    "\n",
    "    # Stop when there's only a single edge left (not possible to merge anything else)\n",
    "    while (m >= 2):\n",
    "        \n",
    "        new_edges = [] \n",
    "        for i in range(0, m, 1):\n",
    "            e1 = edges[i]\n",
    "            for j in range(i + 1, m, 1):\n",
    "                e2 = edges[j]\n",
    "                common_heads = e1[1].intersection(e2[1])\n",
    "                if (len(common_heads) > 0):\n",
    "                    \n",
    "                    # The new edge\n",
    "                    new_tails = e1[0].union(e2[0]) \n",
    "                    new_edge = ([new_tails, common_heads])\n",
    "                    new_edges.append(new_edge)\n",
    "\n",
    "                    # Updating old edges (their head sets)\n",
    "                    e1[1] = set(e1[1] - common_heads)\n",
    "                    e2[1] = set(e2[1] - common_heads)\n",
    "\n",
    "        \n",
    "        # TODO: weight the edges?\n",
    "        # Add all edges with non-null head sets to the incidence matrix\n",
    "        for edge in edges:\n",
    "            col = np.zeros(n_genes)\n",
    "            if len(edge[1]) > 0:\n",
    "                for tail in edge[0]:\n",
    "                    col[tail] = 1 #TODO: TESTING\n",
    "                for head in edge[1]:\n",
    "                    col[head] = 1\n",
    "\n",
    "                B.append(np.copy(col))\n",
    "        \n",
    "        edges = deepcopy(new_edges)\n",
    "        m = len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Creates a hypergraph, represented using an incidence matrix, from the given cell's PW gene network. \n",
    "cell_index: the index into the PWscores array for the desired cell.\n",
    "p: percentage parameter to use in tolerance calculation; decides which relationships relative to the maximum PW TE. \n",
    "returns: B the incidence matrix of the constructed hypergraph. \n",
    "\"\"\"\n",
    "def construct_hyper_graph(cell_index, p):\n",
    "    # init\n",
    "    unique_genes = set()\n",
    "    cell_nw = PWscores[cell_index, :, :]\n",
    "\n",
    "    # The incidence matrix (might use different data structure)\n",
    "    B = []\n",
    "\n",
    "    # all scores within whatever % of the max\n",
    "    tolerance = np.max(cell_nw) * p\n",
    "\n",
    "    tails = set(np.arange(start=0, stop=n_genes, step=1))\n",
    "\n",
    "    # debug var\n",
    "    count = 0\n",
    "\n",
    "    # TODO: TEST IT ALL \n",
    "    while (len(tails) > 0):\n",
    "        edges = []\n",
    "        new_tails = set()\n",
    "        heads_to_remember = set()\n",
    "\n",
    "        for tail in tails:\n",
    "            heads = set(np.flatnonzero(cell_nw[tail, :] > tolerance))\n",
    "            \n",
    "            # Termination condition: once there's no new heads added this never gets hit, no new tails get added and the tail set becomes null at the end of while itr\n",
    "            if (len(heads) > 0) and (heads.isdisjoint(unique_genes)):\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                for head in heads:\n",
    "                    # error handling\n",
    "                    if (cell_nw[tail, head] <= tolerance):\n",
    "                        print(\"BIG BUG\") # TODO: keep this error cond here and handle it better!\n",
    "\n",
    "                    # update the next-level tails\n",
    "                    new_tails.add(head)\n",
    "                    \n",
    "                    # TODO: maybe make the set of all heads for this level here and wait till the end of the iteration to deep copy it over?\n",
    "                    heads_to_remember.add(head)\n",
    "                    \n",
    "                # update the edge set for this hgraph level\n",
    "                tail_as_set = set()\n",
    "                tail_as_set.add(tail)\n",
    "                edges.append([tail_as_set, heads])\n",
    "        \n",
    "        # update the unique heads set\n",
    "        # for e in edges:\n",
    "        #     for h in e[1]:\n",
    "        #         unique_genes.add(h)\n",
    "        for h in heads_to_remember:\n",
    "            unique_genes.add(h)\n",
    "\n",
    "        merge_edges(edges, B)\n",
    "        tails = set(deepcopy(new_tails))\n",
    "    \n",
    "    return B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedges = []\n",
    "# TODO: Test proper via some contrived examples\n",
    "for i in range(n_cells):\n",
    "    G = construct_hyper_graph(i, 0.7)\n",
    "    if (len(G) > 0):\n",
    "        hedges.append(np.transpose(np.array(G)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the edges into one big hypergraph for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2065)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.concatenate(hedges, axis=1)\n",
    "H.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering to determine pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # TODO: calculate using Adjacency and Degree matrices?\n",
    "# L = H @ np.transpose(H)\n",
    "# eigvals, eigvecs = np.linalg.eig(L)\n",
    "\n",
    "# # testing to make sure all eigenvalues are real and non-negative\n",
    "# if (np.sum(np.iscomplex(eigvals)) > 0) or (np.sum(eigvals < 0) > 0):\n",
    "#     raise ValueError(\"Laplacian is not a positive semidefinite matrix\")\n",
    "\n",
    "# # TODO: test and debug this piece! sort based on eigenvalues\n",
    "# vecs = eigvecs[:,np.argsort(eigvals)]\n",
    "# vals = eigvals[np.argsort(eigvals)]\n",
    "\n",
    "# # print(vals)\n",
    "# # # TODO: this approach requires normalized Laplacian\n",
    "# # k = vals.shape[0] - np.sum(vals  0)\n",
    "# k = 4\n",
    "# # TODO: Fit from the first non-zero eigenvalue? Need to do this?\n",
    "# index = np.where(vals > 0)[0][0] \n",
    "\n",
    "# kmeans = KMeans(n_clusters=k)\n",
    "# kmeans.fit(vecs[:,])\n",
    "# np.unique(kmeans.labels_, return_counts=True) # clusters and the number of genes assigned to each cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spectral Clustering Proper\n",
    "T = Dv**-0.5 * (H * W * De**-1 * H.T) * Dv**-0.5 TODO: maybe don't ignore hedge weights, so refactor and do all that proper? First this though.\n",
    "Delta = I - T\n",
    "L_normalized = 0.5 * (I - (Dvis @ A @ Dvis)) Dvis = inverse square root of vertex Degree matrix\n",
    "A = adjacency matrix = H @ W @ H.T - Dv\n",
    "Dv is a diagonal matrix containing vertex degrees, i.e. number of edges the vertex appears in (vertices x vertices) \n",
    "H = incidence matrix\n",
    "W = diagonal matrix of hyperedge weights (hyperedges x hyperedges)\n",
    "De = Diagonal matrix of hyperedge degrees, i.e. the number of vertices in each hyper edge (hedges x hedges) \n",
    "\"\"\"\n",
    "\n",
    "# pruning TFs (because of how python works, it makes more sense to first figure out all the indices where to delete, then delete)\n",
    "count = 0\n",
    "to_prune = []\n",
    "gene_labels = df_temp.columns.to_numpy()\n",
    "\n",
    "for v in range(n_genes):\n",
    "    deg_v = np.sum(H[v,:])\n",
    "    if deg_v == 0:\n",
    "        # remove the vertex, since it isn't involved in any hedges, therefore won't be in any pathways\n",
    "        count += 1\n",
    "        to_prune.append(v)\n",
    "\n",
    "n_genes = n_genes - count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pruned_labels = np.delete(gene_labels, to_prune)\n",
    "pruned_H = np.delete(H, to_prune, axis=0) # delete rows with indices in to_prune\n",
    "\n",
    "# Checking it did its job\n",
    "if np.intersect1d(pruned_labels, gene_labels[to_prune]).shape[0] != 0:\n",
    "    raise ValueError(\"Pruning of background genes failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Diagonal vertex and edge degree matrix  \n",
    "Dv = np.zeros((n_genes, n_genes))\n",
    "De = np.zeros((pruned_H.shape[1], pruned_H.shape[1]))\n",
    "\n",
    "\n",
    "for v in range(n_genes):\n",
    "    deg_v = np.sum(pruned_H[v,:])\n",
    "    if deg_v == 0:\n",
    "        raise ValueError(\"Pruning of background genes failed.\")\n",
    "    else:\n",
    "        Dv[v, v] = deg_v\n",
    "\n",
    "for e in range(pruned_H.shape[1]):\n",
    "    De[e, e] = np.sum(pruned_H[:, e])\n",
    "\n",
    "# Weights for each hyperedge = number of vertices in that edge, just need W to keep that sum\n",
    "# W = \n",
    "\n",
    "# Adjacency matrix of the hypergraph\n",
    "A = (pruned_H @ pruned_H.T) - Dv\n",
    "\n",
    "# Normalized Laplacian of the hypergraph \n",
    "Dvis = np.linalg.inv(np.sqrt(Dv)) # replace each diagonal entry with the reciprocal of its square root, i.e. di with 1/sqrt(di)\n",
    "L_normal = np.eye(n_genes) - (Dvis @ pruned_H @ np.linalg.inv(De) @ (pruned_H.T) @ Dvis) # TODO: add W matrix later\n",
    "L = Dv - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.500145032286355e-16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sanity checking A, D, and L\n",
    "Dv = number of hyperedges a vertex appears in\n",
    "A = is the adjacency matrix: vertex x vertex, 1 if the edge exits (i.e. the two vertices are adjacent)\n",
    "  = For a Hypergraph, that's a tensor, i.e. avoid 'em\n",
    "L = Laplacian, classically defined as Dv - A; properties: all non-neg real eigenvals, every row sums to 0, always has a 0 eigenval with eigenvector of all constants\n",
    "\"\"\"\n",
    "eigvals, eigvecs = np.linalg.eig(L_normal) # L-norm look really good!\n",
    "# eigvals[np.where(eigvals < 0.5)[0]]\n",
    "# eigvals[eigvals < 0.3]\n",
    "# eigvecs[55]\n",
    "np.sum(eigvals < 0)\n",
    "np.argmin(eigvals)\n",
    "# eigvecs[:,0] are the eigenvecs the rows or columns of this matrix? They're the columns\n",
    "np.sum(L_normal * eigvecs[:,0]) # that's pree much 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.11721798e-02,  1.38993693e-02, -6.86490122e-03,  5.57677819e-02,\n",
       "        9.35075824e-02, -3.99601165e-02, -9.49147884e-02,  6.37455604e-01,\n",
       "        1.18984792e-02, -3.90418358e-02,  3.63638894e-02,  5.74964334e-04,\n",
       "       -3.04356350e-02, -6.88835841e-01, -1.77071600e-02, -2.51695616e-02,\n",
       "        1.23331230e-03, -4.09342677e-02, -6.35831316e-03,  1.08419182e-02,\n",
       "        6.97520283e-03,  4.36746314e-03,  2.48797106e-03, -4.47106367e-03,\n",
       "        3.89569572e-03, -1.15139070e-02, -1.42368518e-03,  1.31583950e-02,\n",
       "        5.80380382e-02,  3.56794868e-03, -1.78712593e-03,  1.33670695e-03,\n",
       "       -7.72215055e-03, -8.54178767e-04,  5.85760909e-02, -4.02597233e-03,\n",
       "        2.65879916e-02,  8.44576395e-03, -1.09484160e-02, -6.23052400e-02,\n",
       "       -9.43920889e-03,  1.44087263e-02,  1.98140253e-03,  1.73590587e-03,\n",
       "       -3.46150660e-03,  4.80638440e-02,  7.86929532e-03, -8.30313711e-02,\n",
       "       -2.42196090e-02,  3.37935692e-02,  5.70344493e-03,  5.88750805e-03,\n",
       "        2.27820296e-01, -1.01083820e-02, -5.81331207e-02,  2.71035327e-03,\n",
       "       -1.04277779e-02, -5.70905924e-03,  2.10656387e-03,  2.23760655e-03,\n",
       "       -1.65832261e-02,  4.26945845e-03, -7.08197524e-03,  6.49126272e-03,\n",
       "       -5.71486933e-03,  9.19259217e-03, -3.13322839e-02,  2.19988573e-02,\n",
       "        1.07184679e-03,  9.51180646e-03, -1.17429501e-03, -3.50679589e-03,\n",
       "       -1.76642529e-03, -7.86626579e-02, -4.45296871e-02, -7.27147739e-03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Time to cluster!\n",
    "\n",
    "# sort eigenvals, then sort eigenvecs by the eigenvals\n",
    "sorted_eigvals = np.sort(eigvals)\n",
    "sorted_eigvecs = eigvecs[:,np.argsort(eigvals)] # sorts the columns\n",
    "\n",
    "# TODO: gotta read up more on how this k means version works\n",
    "k = 10\n",
    "# TODO: Fit from the first non-zero eigenvalue? Need to do this?\n",
    "index = np.where(vals > 0)[0][0] \n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(vecs[:,])\n",
    "\n",
    "# assign labels\n",
    "\n",
    "\n",
    "# visualize the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.11721798e-02,  1.38993693e-02, -6.86490122e-03,  5.57677819e-02,\n",
       "        9.35075824e-02, -3.99601165e-02, -9.49147884e-02,  6.37455604e-01,\n",
       "        1.18984792e-02, -3.90418358e-02,  3.63638894e-02,  5.74964334e-04,\n",
       "       -3.04356350e-02, -6.88835841e-01, -1.77071600e-02, -2.51695616e-02,\n",
       "        1.23331230e-03, -4.09342677e-02, -6.35831316e-03,  1.08419182e-02,\n",
       "        6.97520283e-03,  4.36746314e-03,  2.48797106e-03, -4.47106367e-03,\n",
       "        3.89569572e-03, -1.15139070e-02, -1.42368518e-03,  1.31583950e-02,\n",
       "        5.80380382e-02,  3.56794868e-03, -1.78712593e-03,  1.33670695e-03,\n",
       "       -7.72215055e-03, -8.54178767e-04,  5.85760909e-02, -4.02597233e-03,\n",
       "        2.65879916e-02,  8.44576395e-03, -1.09484160e-02, -6.23052400e-02,\n",
       "       -9.43920889e-03,  1.44087263e-02,  1.98140253e-03,  1.73590587e-03,\n",
       "       -3.46150660e-03,  4.80638440e-02,  7.86929532e-03, -8.30313711e-02,\n",
       "       -2.42196090e-02,  3.37935692e-02,  5.70344493e-03,  5.88750805e-03,\n",
       "        2.27820296e-01, -1.01083820e-02, -5.81331207e-02,  2.71035327e-03,\n",
       "       -1.04277779e-02, -5.70905924e-03,  2.10656387e-03,  2.23760655e-03,\n",
       "       -1.65832261e-02,  4.26945845e-03, -7.08197524e-03,  6.49126272e-03,\n",
       "       -5.71486933e-03,  9.19259217e-03, -3.13322839e-02,  2.19988573e-02,\n",
       "        1.07184679e-03,  9.51180646e-03, -1.17429501e-03, -3.50679589e-03,\n",
       "       -1.76642529e-03, -7.86626579e-02, -4.45296871e-02, -7.27147739e-03])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_eigvecs[:, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Eigenvalues for Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.62736121e-04,  1.43412089e-01, -3.38885739e-02, -8.45916279e-02,\n",
       "        2.16501394e-02,  5.71844298e-02, -9.98682743e-02,  5.80163531e-02,\n",
       "        1.81195810e-01, -6.68441750e-02, -7.38993784e-02,  1.41321318e-01,\n",
       "        2.42739312e-02,  6.35017863e-02,  7.36344354e-02, -5.45729489e-02,\n",
       "        5.58404987e-02,  1.10501587e-02,  7.20370780e-02, -7.78206264e-02,\n",
       "        3.19246967e-01, -9.84412544e-03, -1.03354881e-01,  6.57086561e-03,\n",
       "       -7.20045662e-02,  4.14004926e-02, -2.23165727e-02, -3.22628083e-01,\n",
       "       -9.04463870e-02, -4.26938684e-03, -2.40326234e-02, -5.75283604e-02,\n",
       "        3.03606768e-02, -8.09847577e-03,  4.79136191e-03,  9.31445503e-02,\n",
       "        9.62451271e-04,  8.81871615e-03, -1.19701263e-01, -1.81837739e-02,\n",
       "       -5.83033030e-02,  3.57901821e-02,  1.92190934e-02,  2.68810379e-01,\n",
       "        8.31259519e-02,  3.91983068e-03, -2.58258200e-01,  3.74066337e-02,\n",
       "        2.71628271e-02,  1.21667172e-01,  1.91625628e-02,  3.04732871e-04,\n",
       "        1.75004014e-02,  3.74346224e-02,  3.66073247e-02,  9.00906988e-03,\n",
       "       -5.09233675e-02,  3.59467950e-02, -1.45415873e-01, -2.11877420e-01,\n",
       "       -4.94419728e-02,  4.12760960e-01,  4.67322299e-03, -3.19397537e-02,\n",
       "       -2.76222530e-01, -8.13488081e-04, -7.93666539e-02,  7.52221582e-02,\n",
       "       -5.41230295e-02, -3.18479834e-02,  2.73902128e-01,  9.46341042e-02,\n",
       "       -7.40153155e-02,  1.53684371e-02, -9.44038344e-04,  7.76586327e-02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.arange(eigvals.shape[0])\n",
    "# y = np.flip(eigvals)\n",
    "# plt.scatter(x, y)\n",
    "# np.sum((eigvals > 0) and (eigvals < 5))\n",
    "# nonzero_eigvals = eigvals[np.where(eigvals > 0)[0]]\n",
    "# final_eigvals = nonzero_eigvals[np.where(nonzero_eigvals < 5)[0]]\n",
    "# final_eigvals\n",
    "# fiedler_val = np.where(eigvals > 0)[0][-1]\n",
    "# fiedler_vec = eigvecs[:,fiedler_val] \n",
    "# fiedler_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# gene_labels[0] == 'SOX2'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# num_clusters = np.unique(kmeans.labels_).shape[0] #want a set for each cluster\u001b[39;00m\n\u001b[1;32m      5\u001b[0m clusters \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[1;32m      7\u001b[0m     genes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margwhere(kmeans\u001b[39m.\u001b[39mlabels_ \u001b[39m==\u001b[39m i)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m genes\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "# gene_labels = df_temp.columns.to_numpy()\n",
    "# # gene_labels[0] == 'SOX2'\n",
    "# # num_clusters = np.unique(kmeans.labels_).shape[0] #want a set for each cluster\n",
    "\n",
    "# clusters = []\n",
    "# for i in range(k):\n",
    "#     genes = np.argwhere(kmeans.labels_ == i).flatten()\n",
    "#     if genes.shape[0] > 1:\n",
    "#         clusters.append(genes)\n",
    "\n",
    "# for c in clusters:\n",
    "#         print(\"here\")\n",
    "#         # print(gene_labels[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # sn.histplot(eigs)\n",
    "# # #456 vectors, each one has a set of values, I want a count for each unique value\n",
    "# eigvals[eigvals < 1] = 0\n",
    "# # eigvals = np.argwhere(eigvals > 10)\n",
    "# bins, counts = np.unique(eigvals, return_counts=True) # tuple for each eigen value: the value and its count\n",
    "# # # sn.histplot(x=hist[1], y=hist[0])\n",
    "# # # plt.pyplot.hist(bins, weights=counts)\n",
    "# plt.hist(bins[:], color = 'blue', edgecolor = 'black', weights=counts[:])\n",
    "# plt.xlabel(\"eigenvalues\")\n",
    "# plt.ylabel(\"counts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
